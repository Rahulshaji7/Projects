{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75998b9",
   "metadata": {},
   "source": [
    "## I am part of a team for building a pipeline which will predict the next word which will be used by a social media application called SNIPCHAT. As a data scientist, I will build a model using Recurrent Neural Network and will train and test the model which can be used in the application in the future.\n",
    "First I will import all the necessary libraries, then load the dataset which is a novel called sherlock holmes.\n",
    "After that i will clean the dataset, remove unnecessary characters and then tokenize it. Then i will do the feature engineering part. After that I will use Bidirectional LSTM model. After buildig the model,  I will train the model with 20 epochs.And then test the model for next word predcition.\n",
    "\n",
    "\n",
    "\n",
    "https://thecleverprogrammer.com/2020/07/20/next-word-prediction-model/\n",
    "\n",
    "https://www.kaggle.com/code/ysthehurricane/next-word-prediction-bi-lstm-tutorial-easy-way/data - DATASET\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc90e85",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3580166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 11:28:24.550548: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-22 11:28:24.550576: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import heapq\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.metrics.pairwise\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.python.keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0135fc",
   "metadata": {},
   "source": [
    "### LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ca2d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>image</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.png</td>\n",
       "      <td>850</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.png</td>\n",
       "      <td>1100</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>3.png</td>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>One example of building neural…</td>\n",
       "      <td>5.jpeg</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  \\\n",
       "0   1  https://towardsdatascience.com/a-beginners-gui...   \n",
       "1   2  https://towardsdatascience.com/hands-on-graph-...   \n",
       "2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3   4  https://towardsdatascience.com/databricks-how-...   \n",
       "4   5  https://towardsdatascience.com/a-step-by-step-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Files in CSV on Your L...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                  subtitle   image  claps responses  \\\n",
       "0                                      NaN   1.png    850         8   \n",
       "1                                      NaN   2.png   1100        11   \n",
       "2         A Grammar of Graphics for Python   3.png    767         1   \n",
       "3  When I work on Python projects dealing…  4.jpeg    354         0   \n",
       "4          One example of building neural…  5.jpeg    211         3   \n",
       "\n",
       "   reading_time           publication        date  \n",
       "0             8  Towards Data Science  2019-05-30  \n",
       "1             9  Towards Data Science  2019-05-30  \n",
       "2             5  Towards Data Science  2019-05-30  \n",
       "3             4  Towards Data Science  2019-05-30  \n",
       "4             4  Towards Data Science  2019-05-30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../datasets/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ef12a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records :  6508\n",
      "fields :  10\n"
     ]
    }
   ],
   "source": [
    "print(\"records : \",df.shape[0])\n",
    "print(\"fields : \",df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43dd0b",
   "metadata": {},
   "source": [
    "We have 6508 number of records and 10 fields. But I will only use title coloumn for predicting next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce22e5a",
   "metadata": {},
   "source": [
    "## PRINTING COLUMN NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506136dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'url', 'title', 'subtitle', 'image', 'claps', 'responses',\n",
      "       'reading_time', 'publication', 'date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343efa5f",
   "metadata": {},
   "source": [
    "## DISPALYING TITLE OF ARTICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff967fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A Beginner’s Guide to Word Embedding with Gens...\n",
       "1       Hands-on Graph Neural Networks with PyTorch & ...\n",
       "2                            How to Use ggplot2 in Python\n",
       "3       Databricks: How to Save Files in CSV on Your L...\n",
       "4       A Step-by-Step Implementation of Gradient Desc...\n",
       "                              ...                        \n",
       "6503    “We” vs “I” — How Should You Talk About Yourse...\n",
       "6504                     How Donald Trump Markets Himself\n",
       "6505        Content and Marketing Beyond Mass Consumption\n",
       "6506    5 Questions All Copywriters Should Ask Clients...\n",
       "6507               How To Write a Good Business Blog Post\n",
       "Name: title, Length: 6508, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bbcd1",
   "metadata": {},
   "source": [
    "## REMOVING UNWANTED CHARACTERS AND WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0edc5",
   "metadata": {},
   "source": [
    "Since there are many unwanted characters and words, there might be a chance that these can decrease the efficiency of the model. So I am going to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f146ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = df[\"title\"].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
    "df[\"title\"] = df[\"title\"].apply(lambda x: x.replace('\\u200a ',''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d5458",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f44bae",
   "metadata": {},
   "source": [
    "Next I am going to tokeize the text to give unique to all the words to make it into vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14084f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<oov>')  #words which cannot be obtained in word_index\n",
    "tokenizer.fit_on_texts(df['title'])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b618974",
   "metadata": {},
   "source": [
    "## BAG OF NGRAM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5133094",
   "metadata": {},
   "source": [
    "Now I am going to convert titles to sequences and then use the n gram model to do good predcition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bac1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input sequences:  47594\n"
     ]
    }
   ],
   "source": [
    "input_sequences = []\n",
    "for l in df['title']:\n",
    "    token = tokenizer.texts_to_sequences([l])[0]\n",
    "   \n",
    "    \n",
    "    for i in range(1, len(token)):\n",
    "        n_gram = token[:i+1]\n",
    "        input_sequences.append(n_gram)\n",
    "\n",
    "\n",
    "print(\"Total input sequences: \", len(input_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c40a86",
   "metadata": {},
   "source": [
    "here I have displayed the total input sequence of words whcih is 47594"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931af20",
   "metadata": {},
   "source": [
    "## make all the titles into same length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a675c",
   "metadata": {},
   "source": [
    "So to make all the titles into same length I will be using padding to do that process. That is I will find the title with maximum length and then pad other titles accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d77ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence, padding='pre'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bac40",
   "metadata": {},
   "source": [
    "Here maxlen is used to get maximum line length and padding is set to pre because we want to make sure the hidden state output is correct. If we use post-padding, final hidden state will be eliminated so it would be mostly [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474004f",
   "metadata": {},
   "source": [
    "## Splitting feature and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d28e3c",
   "metadata": {},
   "source": [
    "Here I will define some functions for storing features and their corresponding labels. For that I will consider the last elemenst of the sequence as labels and then use onehot encoding on the labels to convert categorical variables as binary vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506e7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,lab = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "y = tf.keras.utils.to_categorical(lab, num_classes = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c622a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5 665  65\n",
      "   2]\n",
      "0.0\n",
      "448\n"
     ]
    }
   ],
   "source": [
    "print(x[3])\n",
    "print(y[3][10])\n",
    "print(lab[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe3d29b",
   "metadata": {},
   "source": [
    "here i have stores features and labels and have displayed them as arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cafb07",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c577c",
   "metadata": {},
   "source": [
    "Now I am going to use Bidirectional LSTM  to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc716fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 11:28:33.403016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-22 11:28:33.403052: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-22 11:28:33.403083: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Ubuntu-1804-bionic-64-minimal): /proc/driver/nvidia/version does not exist\n",
      "2022-12-22 11:28:33.403370: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2022-12-22 11:28:34.453844: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1488/1488 [==============================] - 54s 35ms/step - loss: 6.8135 - accuracy: 0.1261\n",
      "Epoch 2/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 5.7907 - accuracy: 0.1784\n",
      "Epoch 3/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 4.9145 - accuracy: 0.2102\n",
      "Epoch 4/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 4.1662 - accuracy: 0.2551\n",
      "Epoch 5/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 3.6061 - accuracy: 0.3064\n",
      "Epoch 6/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 3.2477 - accuracy: 0.3495\n",
      "Epoch 7/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.9967 - accuracy: 0.3781\n",
      "Epoch 8/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.8323 - accuracy: 0.4007\n",
      "Epoch 9/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.6707 - accuracy: 0.4246\n",
      "Epoch 10/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.5477 - accuracy: 0.4435\n",
      "Epoch 11/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.4903 - accuracy: 0.4516\n",
      "Epoch 12/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.4485 - accuracy: 0.4592\n",
      "Epoch 13/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.4181 - accuracy: 0.4608\n",
      "Epoch 14/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.3501 - accuracy: 0.4718\n",
      "Epoch 15/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.3071 - accuracy: 0.4812\n",
      "Epoch 16/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.2739 - accuracy: 0.4841\n",
      "Epoch 17/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.2749 - accuracy: 0.4867\n",
      "Epoch 18/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.2333 - accuracy: 0.4945\n",
      "Epoch 19/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.2072 - accuracy: 0.4993\n",
      "Epoch 20/20\n",
      "1488/1488 [==============================] - 52s 35ms/step - loss: 2.2176 - accuracy: 0.4975\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fefaef41760>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history = model.fit(x, y, epochs=20, verbose=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cdb45",
   "metadata": {},
   "source": [
    "Here I have added embedding layer and dense layer to the model. As paramters I used size of the vocabulary,dimension of dense layer and set activation as softmax so that the activation for the last layer of a classification network could be interpreted as a probability distribution. And used adam as optimizer, I also displayed the accuracy of eah epochs till 20 of the validation. Since there are 2 or more output lables, I have used categorical_crossentropy as loass fucntion for the multi class classification model. And I set verbose as 1 just to show how my output sould look like, I can aslo set it as 0,2 or 3. And for adam I set the learning rate as 0.01 which depends on how fast or slow the model trains. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ad8a3",
   "metadata": {},
   "source": [
    "## TESTING THE MODEL FOR PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a486ad6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy and rejection is the\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"happy and\"\n",
    "next_words = 3\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence-1, padding='pre')\n",
    "    predict = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predict:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea641a",
   "metadata": {},
   "source": [
    "Here the seed_text and next_words can be changed according to the prediction process. Currently I set seed_text as \"use of\" and next_words to 5 and the ouput I got is \"use of the stars episode i use\". If I change the seed_text and next_words to \"happy and\" and 3 respectively the predicted output is \"happy and rejection is the\". \n",
    "\n",
    "The model can be used for not only app but for many search engines like google, brave etc. And with minor upgrades, the model holds many future opportunities for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ced73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
